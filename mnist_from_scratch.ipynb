{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tqdm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[86], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m trange\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(url):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpathlib\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mrequests\u001b[39;00m\u001b[38;5;241m,\u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mhashlib\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mgzip\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mtempfile\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tqdm'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import trange\n",
    "\n",
    "def fetch(url):\n",
    "    import pathlib, requests,os, hashlib, numpy, gzip, tempfile\n",
    "    fp = os.path.join(tempfile.gettempdir(), hashlib.md5(url.encode(\"utf-8\")).hexdigest())\n",
    "    if(os.path.isfile(fp)):\n",
    "        with open(fp, \"rb\") as f:\n",
    "            dat = f.read()\n",
    "    else:\n",
    "        with open(fp, \"wb\") as f:\n",
    "            dat = requests.get(url).content\n",
    "            f.write(dat)\n",
    "    return numpy.frombuffer(gzip.decompress(dat), dtype=numpy.uint8).copy()\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "#fetch data\n",
    "X_train = fetch(\"https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\")\n",
    "Y_train = fetch(\"https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\")\n",
    "X_test = fetch(\"https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\")\n",
    "Y_test = fetch(\"https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\")\n",
    "\n",
    "#normalize pixel\n",
    "def preprocess_mnist(data):\n",
    "    data = data[16:]   \n",
    "    images = data.reshape(-1, 28, 28) \n",
    "    images = images.astype('float32') / 255.0\n",
    "    \n",
    "    return images\n",
    "\n",
    "X_train_normalized = preprocess_mnist(X_train).reshape(-1,784)\n",
    "X_test_normalized = preprocess_mnist(X_test)\n",
    "\n",
    "#one-hot encoding\n",
    "def one_hot_encoding(data):\n",
    "    data = data[8:]\n",
    "    \n",
    "    n_labels = len(data)\n",
    "    one_hot = np.zeros((n_labels, 10), dtype=np.int8)\n",
    "    \n",
    "    one_hot[np.arange(n_labels), data] = 1\n",
    "    \n",
    "    return one_hot\n",
    "    \n",
    "X_train = one_hot_encoding(Y_train)\n",
    "Y_train = one_hot_encoding(Y_train)\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.rng = np.random.default_rng()\n",
    "        \n",
    "        # Initialisierung der Gewichte mit der layer_init Funktion\n",
    "        self.w1 = self.layer_init(input_size, hidden_size)\n",
    "        self.w2 = self.layer_init(hidden_size, output_size)\n",
    "        \n",
    "        self.b1 = np.zeros((1, hidden_size), dtype=np.float32)\n",
    "        self.b2 = np.zeros((1, output_size), dtype=np.float32)\n",
    "\n",
    "    def layer_init(self, m, h):\n",
    "        # Gleichverteilte Initialisierung\n",
    "        ret = self.rng.uniform(-1., 1., size=(m, h)) / np.sqrt(m * h)\n",
    "        return ret.astype(np.float32)\n",
    "\n",
    "    @staticmethod\n",
    "    def relu(x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.z1 = np.dot(x,self.w1) + self.b1\n",
    "        self.a1 = self.relu(self.z1)\n",
    "        self.z2 = np.dot(self.a1, self.w2) + self.b2\n",
    "        output = self.softmax(self.z2)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def softmax(x):\n",
    "        exp = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exp/np.sum(exp, axis=1, keepdims=True)\n",
    "\n",
    "    def backward(self, x, y, output):\n",
    "        m = y.shape[0]\n",
    "        dz2 = output - y\n",
    "        dW2 = np.dot(self.a1.T, dz2) / m\n",
    "        db2 = np.sum(dz2, axis=0, keepdims=True) / m\n",
    "        dz1 = np.dot(dz2, self.w2.T) * (self.a1 > 0)\n",
    "        dW1 = np.dot(x.T, dz1) / m\n",
    "        db1 = np.sum(dz1, axis=0, keepdims=True) / m\n",
    "\n",
    "        # Gewichte aktualisieren\n",
    "        learning_rate = 0.01\n",
    "        self.w1 -= learning_rate * dW1\n",
    "        self.b1 -= learning_rate * db1\n",
    "        self.w2 -= learning_rate * dW2\n",
    "        self.b2 -= learning_rate * db2\n",
    "\n",
    "    def train(self, x_train, y_train, epochs):\n",
    "        for epoch in (t := trange(epochs)):\n",
    "            output = self.forward(x_train)\n",
    "            loss = self.cross_entropy(output, y_train)\n",
    "            self.backward(x_train, y_train, output)\n",
    "\n",
    "            t.set_description(\"Epoch: % Loss: %\" % (epoch, loss))\n",
    "        \n",
    "    def cross_entropy(self, predictions, targets):\n",
    "        predictions = np.clip(predictions, 1e-12, 1. - 1e-12)\n",
    "        N = predictions.shape[0]\n",
    "        loss = -np.sum(targets * np.log(predictions)) / N\n",
    "        return loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, loss: 2.3024680614471436\n",
      "Epoch: 100, loss: 2.281738042831421\n",
      "Epoch: 200, loss: 2.2420923709869385\n",
      "Epoch: 300, loss: 2.1663458347320557\n",
      "Epoch: 400, loss: 2.0303144454956055\n",
      "Epoch: 500, loss: 1.8203396797180176\n",
      "Epoch: 600, loss: 1.555733323097229\n",
      "Epoch: 700, loss: 1.2994908094406128\n",
      "Epoch: 800, loss: 1.0973271131515503\n",
      "Epoch: 900, loss: 0.9495136141777039\n"
     ]
    }
   ],
   "source": [
    "nn = Model(784, 128, 10)\n",
    "nn.train(X_train_normalized, Y_train, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
