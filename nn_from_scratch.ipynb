{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.5033238083516929\n",
      "Epoch 1000, Loss: 0.5008723968133092\n",
      "Epoch 2000, Loss: 0.4986143015672444\n",
      "Epoch 3000, Loss: 0.4828344532652319\n",
      "Epoch 4000, Loss: 0.37997511657965133\n",
      "Epoch 5000, Loss: 0.24422976066877516\n",
      "Epoch 6000, Loss: 0.19825694549664097\n",
      "Epoch 7000, Loss: 0.18082178493630133\n",
      "Epoch 8000, Loss: 0.17204073699955913\n",
      "Epoch 9000, Loss: 0.16681708441878138\n",
      "Epoch 10000, Loss: 0.16337159379881253\n",
      "Epoch 11000, Loss: 0.1609353537996224\n",
      "Epoch 12000, Loss: 0.1591247602117587\n",
      "Epoch 13000, Loss: 0.15772788792528752\n",
      "Epoch 14000, Loss: 0.1566184170078409\n",
      "Epoch 15000, Loss: 0.15571649893298434\n",
      "Epoch 16000, Loss: 0.15496924462269446\n",
      "Epoch 17000, Loss: 0.1543402616636338\n",
      "Epoch 18000, Loss: 0.15380370375343655\n",
      "Epoch 19000, Loss: 0.15334071779969988\n",
      "Epoch 20000, Loss: 0.15293723413443974\n",
      "Epoch 21000, Loss: 0.1525825434084807\n",
      "Epoch 22000, Loss: 0.15226835216847776\n",
      "Epoch 23000, Loss: 0.15198813948310363\n",
      "Epoch 24000, Loss: 0.15173670844849457\n",
      "Epoch 25000, Loss: 0.15150986709901335\n",
      "Epoch 26000, Loss: 0.15130419721334593\n",
      "Epoch 27000, Loss: 0.15111688404221316\n",
      "Epoch 28000, Loss: 0.15094558903819946\n",
      "Epoch 29000, Loss: 0.1507883534436113\n",
      "Epoch 30000, Loss: 0.1506435243562799\n",
      "Epoch 31000, Loss: 0.15050969739467976\n",
      "Epoch 32000, Loss: 0.15038567177598705\n",
      "Epoch 33000, Loss: 0.150270414784353\n",
      "Epoch 34000, Loss: 0.15016303341890003\n",
      "Epoch 35000, Loss: 0.15006275158580257\n",
      "Epoch 36000, Loss: 0.1499688916108889\n",
      "Epoch 37000, Loss: 0.14988085914813093\n",
      "Epoch 38000, Loss: 0.1497981307786444\n",
      "Epoch 39000, Loss: 0.1497202437573112\n",
      "Epoch 40000, Loss: 0.14964678748571994\n",
      "Epoch 41000, Loss: 0.14957739638192707\n",
      "Epoch 42000, Loss: 0.1495117438874506\n",
      "Epoch 43000, Loss: 0.14944953740557396\n",
      "Epoch 44000, Loss: 0.14939051400654357\n",
      "Epoch 45000, Loss: 0.14933443676757635\n",
      "Epoch 46000, Loss: 0.14928109164094588\n",
      "Epoch 47000, Loss: 0.14923028476343075\n",
      "Epoch 48000, Loss: 0.14918184013629388\n",
      "Epoch 49000, Loss: 0.1491355976176552\n",
      "Epoch 50000, Loss: 0.14909141117930647\n",
      "Epoch 51000, Loss: 0.149049147388242\n",
      "Epoch 52000, Loss: 0.149008684079854\n",
      "Epoch 53000, Loss: 0.14896990919517422\n",
      "Epoch 54000, Loss: 0.14893271975900038\n",
      "Epoch 55000, Loss: 0.14889702097940458\n",
      "Epoch 56000, Loss: 0.14886272545214507\n",
      "Epoch 57000, Loss: 0.14882975245600816\n",
      "Epoch 58000, Loss: 0.14879802732719422\n",
      "Epoch 59000, Loss: 0.1487674809026021\n",
      "Epoch 60000, Loss: 0.14873804902332802\n",
      "Epoch 61000, Loss: 0.14870967209092137\n",
      "Epoch 62000, Loss: 0.14868229466997818\n",
      "Epoch 63000, Loss: 0.14865586513152784\n",
      "Epoch 64000, Loss: 0.1486303353324151\n",
      "Epoch 65000, Loss: 0.14860566032651207\n",
      "Epoch 66000, Loss: 0.1485817981041376\n",
      "Epoch 67000, Loss: 0.14855870935652352\n",
      "Epoch 68000, Loss: 0.14853635726256528\n",
      "Epoch 69000, Loss: 0.14851470729544008\n",
      "Epoch 70000, Loss: 0.14849372704696406\n",
      "Epoch 71000, Loss: 0.14847338606782456\n",
      "Epoch 72000, Loss: 0.14845365572203667\n",
      "Epoch 73000, Loss: 0.14843450905417116\n",
      "Epoch 74000, Loss: 0.14841592066806622\n",
      "Epoch 75000, Loss: 0.14839786661588056\n",
      "Epoch 76000, Loss: 0.14838032429647643\n",
      "Epoch 77000, Loss: 0.1483632723622285\n",
      "Epoch 78000, Loss: 0.1483466906334574\n",
      "Epoch 79000, Loss: 0.1483305600197689\n",
      "Epoch 80000, Loss: 0.14831486244765868\n",
      "Epoch 81000, Loss: 0.1482995807938082\n",
      "Epoch 82000, Loss: 0.14828469882355733\n",
      "Epoch 83000, Loss: 0.1482702011340904\n",
      "Epoch 84000, Loss: 0.14825607310192182\n",
      "Epoch 85000, Loss: 0.14824230083430462\n",
      "Epoch 86000, Loss: 0.14822887112422667\n",
      "Epoch 87000, Loss: 0.14821577140868736\n",
      "Epoch 88000, Loss: 0.14820298972998153\n",
      "Epoch 89000, Loss: 0.14819051469973926\n",
      "Epoch 90000, Loss: 0.14817833546549655\n",
      "Epoch 91000, Loss: 0.14816644167959125\n",
      "Epoch 92000, Loss: 0.14815482347019793\n",
      "Epoch 93000, Loss: 0.1481434714143324\n",
      "Epoch 94000, Loss: 0.14813237651267103\n",
      "Epoch 95000, Loss: 0.14812153016604457\n",
      "Epoch 96000, Loss: 0.14811092415347876\n",
      "Epoch 97000, Loss: 0.1481005506116621\n",
      "Epoch 98000, Loss: 0.14809040201573737\n",
      "Epoch 99000, Loss: 0.14808047116131437\n",
      "Input: [0, 0], Predicted Output: 0, Validation Output: 0\n",
      "Input: [0, 1], Predicted Output: 1, Validation Output: 1\n",
      "Input: [1, 0], Predicted Output: 1, Validation Output: 1\n",
      "Input: [1, 1], Predicted Output: 0, Validation Output: 0\n"
     ]
    }
   ],
   "source": [
    "from math import exp\n",
    "import random\n",
    "\n",
    "# XOR-Trainingdaten\n",
    "training_data = [[0, 0, 0], [0, 1, 1], [1, 0, 1], [1, 1, 0]]\n",
    "\n",
    "# Gewichte und Biases initialisieren\n",
    "weights_input_hidden = [random.uniform(-1, 1) for _ in range(4)]\n",
    "weights_hidden_output = [random.uniform(-1, 1) for _ in range(2)]\n",
    "bias_hidden = [random.uniform(-1, 1) for _ in range(2)]\n",
    "bias_output = random.uniform(-1, 1)\n",
    "\n",
    "# Sigmoid-Funktion\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + exp(-x))\n",
    "\n",
    "# Verlustfunktion (Mean Squared Error)\n",
    "def mse_loss(output, target):\n",
    "    return 0.5 * (output - target) ** 2\n",
    "\n",
    "# Vorwärtspropagation\n",
    "def forward_propagation(inputs, weights_input_hidden, weights_hidden_output, bias_hidden, bias_output):\n",
    "    hidden_layer = []\n",
    "    for i in range(2):\n",
    "        weighted_sum = inputs[0] * weights_input_hidden[i * 2] + inputs[1] * weights_input_hidden[i * 2 + 1] + bias_hidden[i]\n",
    "        hidden_layer.append(sigmoid(weighted_sum))\n",
    "\n",
    "    output = sigmoid(hidden_layer[0] * weights_hidden_output[0] + hidden_layer[1] * weights_hidden_output[1] + bias_output)\n",
    "    return hidden_layer, output\n",
    "\n",
    "# Rückwärtspropagation\n",
    "def backpropagation(inputs, hidden_layer, output, target, weights_input_hidden, weights_hidden_output, bias_hidden, bias_output, learning_rate=0.1):\n",
    "    output_error = output - target\n",
    "    output_delta = output_error * output * (1 - output)\n",
    "\n",
    "    hidden_errors = []\n",
    "    for i in range(2):\n",
    "        hidden_error = output_delta * weights_hidden_output[i]\n",
    "        hidden_delta = hidden_error * hidden_layer[i] * (1 - hidden_layer[i])\n",
    "        hidden_errors.append(hidden_delta)\n",
    "\n",
    "    # Update Gewichte und Biases (Hidden Layer)\n",
    "    for i in range(2):\n",
    "        weights_input_hidden[i * 2] -= learning_rate * hidden_errors[i] * inputs[0]\n",
    "        weights_input_hidden[i * 2 + 1] -= learning_rate * hidden_errors[i] * inputs[1]\n",
    "        bias_hidden[i] -= learning_rate * hidden_errors[i]\n",
    "\n",
    "    # Update Gewichte und Biases (Output Layer)\n",
    "    for i in range(2):\n",
    "        weights_hidden_output[i] -= learning_rate * output_delta * hidden_layer[i]\n",
    "    bias_output -= learning_rate * output_delta\n",
    "\n",
    "# Trainingsfunktion\n",
    "def train_nn(training_data, epochs=100000):\n",
    "    global bias_output  # Da der Bias in der Rückwärtspropagation geändert wird\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for data in training_data:\n",
    "            inputs = data[:2]\n",
    "            target = data[2]\n",
    "\n",
    "            # Vorwärtspropagation\n",
    "            hidden_layer, output = forward_propagation(inputs, weights_input_hidden, weights_hidden_output, bias_hidden, bias_output)\n",
    "\n",
    "            # Verlust berechnen\n",
    "            loss = mse_loss(output, target)\n",
    "            total_loss += loss\n",
    "\n",
    "            # Rückwärtspropagation\n",
    "            backpropagation(inputs, hidden_layer, output, target, weights_input_hidden, weights_hidden_output, bias_hidden, bias_output)\n",
    "\n",
    "        # Verlust alle 1000 Epochen anzeigen\n",
    "        if epoch % 1000 == 0:\n",
    "            print(f'Epoch {epoch}, Loss: {total_loss}')\n",
    "\n",
    "# Testen des neuronalen Netzwerks\n",
    "def main():\n",
    "    train_nn(training_data)\n",
    "\n",
    "    # Testen des neuronalen Netzwerks\n",
    "    for data in training_data:\n",
    "        inputs = data[:2]\n",
    "        _, output = forward_propagation(inputs, weights_input_hidden, weights_hidden_output, bias_hidden, bias_output)\n",
    "        print(f'Input: {inputs}, Predicted Output: {round(output)}, Validation Output: {data[2]}')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
